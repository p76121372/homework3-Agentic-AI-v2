import os
import subprocess
import sys
from typing import TypedDict
from langgraph.graph import StateGraph, END

# å…¨åŸŸè¨­å®š
MODEL_NAME = "gemini-2.5-flash-lite"

def Use_LLM(prompt_text, temperature=0.7):
    """
    çµ±ä¸€çš„ LLM èª¿ç”¨å‡½æ•¸
    
    Args:
        prompt_text (str): è¦ç™¼é€çµ¦ LLM çš„æ–‡å­—å…§å®¹
        temperature (float): LLM çš„ temperature åƒæ•¸ï¼Œé è¨­ç‚º 0.7
    
    Returns:
        str: LLM çš„å›æ‡‰å…§å®¹ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å›ç©ºå­—ä¸²
    """
    try:
        import os
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain_core.messages import HumanMessage
        
        # è®€å– API Key
        with open('API_key.txt', 'r') as f:
            api_key = f.read().strip()
        os.environ['GOOGLE_API_KEY'] = api_key
        
        # å»ºç«‹ LLM
        llm = ChatGoogleGenerativeAI(
            model=MODEL_NAME,
            temperature=temperature,
            max_retries=2,
            google_api_key=os.environ['GOOGLE_API_KEY'],
        )
        
        # èª¿ç”¨ LLM
        response = llm.invoke([HumanMessage(content=prompt_text)])
        return response.content
        
    except Exception as e:
        print(f"[!] LLM èª¿ç”¨å¤±æ•—: {e}")
        return ""

def analyze_user_intent(user_input, current_num_rows, current_normal_prob, current_abnormal_prob, current_null_prob):
    """è®“ LLM åˆ†æç”¨æˆ¶æ„åœ–ï¼Œåˆ¤æ–·æ˜¯å¦è¦ä¿®æ”¹åƒæ•¸"""
    
    intent_prompt = f"""ä½ æ˜¯åƒæ•¸ä¿®æ”¹æ„åœ–åˆ†æå°ˆå®¶ã€‚è«‹åˆ†æç”¨æˆ¶çš„è¼¸å…¥ï¼Œåˆ¤æ–·æ˜¯å¦è¦ä¿®æ”¹åƒæ•¸ã€‚

ç•¶å‰åƒæ•¸ç‹€æ…‹ï¼š
- num_rows: {current_num_rows}
- normal_prob: {current_normal_prob}
- abnormal_prob: {current_abnormal_prob}
- null_prob: {current_null_prob}

ç”¨æˆ¶è¼¸å…¥ï¼š"{user_input}"

è«‹åˆ†æç”¨æˆ¶æ˜¯å¦æƒ³è¦ä¿®æ”¹ä»»ä½•åƒæ•¸ã€‚å¦‚æœæ˜¯ï¼Œè«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š

[PARAM_UPDATE]
param_name=new_value
param_name=new_value
[/PARAM_UPDATE]

ä¾‹å¦‚ï¼š
[PARAM_UPDATE]
num_rows=500
normal_prob=0.9
[/PARAM_UPDATE]

å¦‚æœç”¨æˆ¶åªæ˜¯è©¢å•å•é¡Œè€Œä¸æ˜¯è¦ä¿®æ”¹åƒæ•¸ï¼Œè«‹æ­£å¸¸å›ç­”å•é¡Œï¼Œä¸è¦åŒ…å« [PARAM_UPDATE] æ¨™ç±¤ã€‚

æ³¨æ„ï¼š
- num_rows å¿…é ˆæ˜¯æ•´æ•¸
- normal_prob, abnormal_prob, null_prob å¿…é ˆæ˜¯ 0-1 ä¹‹é–“çš„å°æ•¸
- åªæœ‰ç•¶ç”¨æˆ¶æ˜ç¢ºè¡¨ç¤ºè¦ä¿®æ”¹åƒæ•¸æ™‚æ‰è¼¸å‡º [PARAM_UPDATE] æ¨™ç±¤"""

    # ä½¿ç”¨çµ±ä¸€çš„ LLM å‡½æ•¸
    return Use_LLM(intent_prompt, temperature=0.7)

def parse_llm_output(llm_response):
    """è§£æ LLM è¼¸å‡ºçš„åƒæ•¸ä¿®æ”¹æŒ‡ä»¤"""
    import re
    
    # æŸ¥æ‰¾ [PARAM_UPDATE] æ¨™ç±¤å…§çš„å…§å®¹
    pattern = r'\[PARAM_UPDATE\](.*?)\[/PARAM_UPDATE\]'
    match = re.search(pattern, llm_response, re.DOTALL)
    
    if not match:
        return {}
    
    param_block = match.group(1).strip()
    param_updates = {}
    
    # è§£ææ¯ä¸€è¡Œçš„åƒæ•¸è¨­å®š
    for line in param_block.split('\n'):
        line = line.strip()
        if '=' in line:
            try:
                param_name, value_str = line.split('=', 1)
                param_name = param_name.strip()
                value_str = value_str.strip()
                
                if param_name == 'num_rows':
                    param_updates[param_name] = int(float(value_str))
                elif param_name in ['normal_prob', 'abnormal_prob', 'null_prob']:
                    value = float(value_str)
                    if 0 <= value <= 1:
                        param_updates[param_name] = value
                    else:
                        print(f"[!] åƒæ•¸ {param_name} çš„å€¼ {value} ä¸åœ¨æœ‰æ•ˆç¯„åœ [0,1] å…§")
            except ValueError as e:
                print(f"[!] ç„¡æ³•è§£æåƒæ•¸è¡Œ: {line}, éŒ¯èª¤: {e}")
    
    return param_updates

# ç‹€æ…‹å®šç¾©
class WorkflowState(TypedDict):
    num_rows: int
    normal_prob: float
    abnormal_prob: float
    null_prob: float
    file_exists: bool
    data_path: str
    data_generated: bool
    output: str
    anomaly_checked: bool
    # æ–°å¢å°è©±ç›¸é—œç‹€æ…‹
    user_input: str
    llm_response: str
    params_confirmed: bool
    conversation_active: bool
    # æ–°å¢ç¸½çµç›¸é—œç‹€æ…‹
    summary_saved: bool
    llm_summary_generated: bool
    # æ–°å¢ç”¨æˆ¶æŸ¥è©¢ç›¸é—œç‹€æ…‹
    user_question: str
    code_flag: str  # "yes" æˆ– "no"
    llm_code_response: str
    code_execution_result: str
    code_error: str
    query_completed: bool
    continue_asking: bool

def check_file(state: WorkflowState, config=None):
    data_path = os.path.join('utils', 'Data', 'testing.csv')
    exists = os.path.exists(data_path)
    state['file_exists'] = exists
    state['data_path'] = data_path
    state['output'] = data_path  # è¨­å®š output è·¯å¾‘
    return state

def prompt_params(state: WorkflowState, config=None):
    """åˆå§‹åŒ–åƒæ•¸è¨­å®šå°è©±"""
    print("\n[!] utils/Data/testing.csv ä¸å­˜åœ¨ï¼Œå•Ÿå‹•æ™ºèƒ½åƒæ•¸è¨­å®šåŠ©æ‰‹...")
    
    # è¨­å®šé è¨­åƒæ•¸
    state['num_rows'] = 300
    state['normal_prob'] = 0.95
    state['abnormal_prob'] = 0.3
    state['null_prob'] = 0.05
    state['output'] = 'utils/Data/testing.csv'
    state['params_confirmed'] = False
    state['conversation_active'] = True
    
    print("=" * 80)
    print(f"""ğŸ¤– æ™ºèƒ½åƒæ•¸è¨­å®šåŠ©æ‰‹å·²å•Ÿå‹•ï¼
ğŸ“‹ ç•¶å‰é è¨­åƒæ•¸ï¼š
   - è³‡æ–™è¡Œæ•¸ (num_rows): {state['num_rows']}
   - æ­£å¸¸è³‡æ–™æ„Ÿæ¸¬å™¨æ­£å¸¸æ©Ÿç‡ (normal_prob): {state['normal_prob']}
   - ç•°å¸¸è³‡æ–™æ„Ÿæ¸¬å™¨æ­£å¸¸æ©Ÿç‡ (abnormal_prob): {state['abnormal_prob']}
   - ç©ºå€¼æ©Ÿç‡ (null_prob): {state['null_prob']}

ğŸ’¬ ä½ å¯ä»¥ï¼š
   - è©¢å•ä»»ä½•é—œæ–¼åƒæ•¸çš„å•é¡Œ
   - ä¿®æ”¹åƒæ•¸ï¼ˆä¾‹å¦‚ï¼š"è¨­å®šè¡Œæ•¸ç‚º500"ï¼‰
   - è¼¸å…¥ 'yes' é–‹å§‹ç”¨ç•¶å‰åƒæ•¸ç”Ÿæˆè³‡æ–™""")
    print("=" * 80)
    
    # ç­‰å¾…ç”¨æˆ¶è¼¸å…¥
    user_input = input("\nä½ : ").strip()
    state['user_input'] = user_input
    
    # æª¢æŸ¥æ˜¯å¦ç¢ºèªåƒæ•¸
    if user_input.lower() in ['yes', 'y', 'ç¢ºå®š', 'é–‹å§‹']:
        state['params_confirmed'] = True
        state['conversation_active'] = False
        print(f"\nâœ… ç¢ºèªä½¿ç”¨åƒæ•¸ç”Ÿæˆè³‡æ–™ï¼")
    
    return state

def user_intent_analysis(state: WorkflowState, config=None):
    """åˆ†æç”¨æˆ¶æ„åœ–ä¸¦è™•ç†åƒæ•¸ä¿®æ”¹"""
    
    try:
        # åˆ†æç”¨æˆ¶æ„åœ–
        intent_analysis = analyze_user_intent(
            state['user_input'], 
            state['num_rows'], 
            state['normal_prob'], 
            state['abnormal_prob'], 
            state['null_prob']
        )
        
        state['llm_response'] = intent_analysis
        
        # è§£æåƒæ•¸ä¿®æ”¹æŒ‡ä»¤
        param_updates = parse_llm_output(intent_analysis)
        if param_updates:
            print("=" * 80)
            for param_name, new_value in param_updates.items():
                if param_name == 'num_rows':
                    state['num_rows'] = new_value
                    print(f"âœ… å·²æ›´æ–°è³‡æ–™è¡Œæ•¸ç‚º: {new_value}")
                elif param_name == 'normal_prob':
                    state['normal_prob'] = new_value
                    print(f"âœ… å·²æ›´æ–°æ­£å¸¸è³‡æ–™æ„Ÿæ¸¬å™¨æ­£å¸¸æ©Ÿç‡ç‚º: {new_value}")
                elif param_name == 'abnormal_prob':
                    state['abnormal_prob'] = new_value
                    print(f"âœ… å·²æ›´æ–°ç•°å¸¸è³‡æ–™æ„Ÿæ¸¬å™¨æ­£å¸¸æ©Ÿç‡ç‚º: {new_value}")
                elif param_name == 'null_prob':
                    state['null_prob'] = new_value
                    print(f"âœ… å·²æ›´æ–°ç©ºå€¼æ©Ÿç‡ç‚º: {new_value}")
            
            print(f"""
ğŸ“‹ æ›´æ–°å¾Œçš„åƒæ•¸ï¼š
   - è³‡æ–™è¡Œæ•¸ (num_rows): {state['num_rows']}
   - æ­£å¸¸è³‡æ–™æ„Ÿæ¸¬å™¨æ­£å¸¸æ©Ÿç‡ (normal_prob): {state['normal_prob']}
   - ç•°å¸¸è³‡æ–™æ„Ÿæ¸¬å™¨æ­£å¸¸æ©Ÿç‡ (abnormal_prob): {state['abnormal_prob']}
   - ç©ºå€¼æ©Ÿç‡ (null_prob): {state['null_prob']}""")
            print("=" * 80)
        else:
            # æ²’æœ‰åƒæ•¸ä¿®æ”¹ï¼Œé¡¯ç¤º LLM å›æ‡‰
            print("=" * 80)
            print(f"ğŸ¤– åŠ©æ‰‹: {intent_analysis}")
            print("=" * 80)
        
        # ç¹¼çºŒå°è©±ï¼Œç­‰å¾…ä¸‹ä¸€å€‹è¼¸å…¥
        user_input = input("\nä½ : ").strip()
        state['user_input'] = user_input
        
        # æª¢æŸ¥æ˜¯å¦ç¢ºèªåƒæ•¸
        if user_input.lower() in ['yes', 'y', 'ç¢ºå®š', 'é–‹å§‹']:
            state['params_confirmed'] = True
            state['conversation_active'] = False
            print("=" * 80)
            print("âœ… ç¢ºèªä½¿ç”¨åƒæ•¸ç”Ÿæˆè³‡æ–™ï¼")
            print("=" * 80)
        
    except Exception as e:
        print(f"[!] LLM åˆ†æå¤±æ•—: {e}")
        # å¦‚æœ LLM å¤±æ•—ï¼Œæ”¹ç”¨ç°¡å–®æ¨¡å¼
        state['conversation_active'] = False
        state['params_confirmed'] = True
        print("æ”¹ç”¨é è¨­åƒæ•¸ç”Ÿæˆè³‡æ–™")
    
    return state

def simple_prompt_params(state: WorkflowState):
    """ç°¡å–®ç‰ˆæœ¬çš„åƒæ•¸è¨­å®šï¼ˆç•¶ LLM ä¸å¯ç”¨æ™‚ï¼‰"""
    print("\n[!] ä½¿ç”¨ç°¡å–®æ¨¡å¼è¨­å®šåƒæ•¸")
    
    try:
        num_rows = input("ç”Ÿæˆè¡Œæ•¸ (num_rows, é è¨­300): ")
        num_rows = int(num_rows) if num_rows.strip() else 300
    except Exception:
        num_rows = 300
    try:
        normal_prob = input("normal label æ­£å¸¸å€¼åŸŸæ©Ÿç‡ (normal_prob, é è¨­0.95): ")
        normal_prob = float(normal_prob) if normal_prob.strip() else 0.95
    except Exception:
        normal_prob = 0.95
    try:
        abnormal_prob = input("abnormal label æ­£å¸¸å€¼åŸŸæ©Ÿç‡ (abnormal_prob, é è¨­0.3): ")
        abnormal_prob = float(abnormal_prob) if abnormal_prob.strip() else 0.3
    except Exception:
        abnormal_prob = 0.3
    try:
        null_prob = input("ç©ºå€¼æ©Ÿç‡ (null_prob, é è¨­0.05): ")
        null_prob = float(null_prob) if null_prob.strip() else 0.05
    except Exception:
        null_prob = 0.05
    
    state['num_rows'] = num_rows
    state['output'] = 'utils/Data/testing.csv'
    state['normal_prob'] = normal_prob
    state['abnormal_prob'] = abnormal_prob
    state['null_prob'] = null_prob
    return state

def generate_data(state: WorkflowState, config=None):
    print("\n[Agentic Workflow] åŸ·è¡Œ generate_data.py ç”¢ç”Ÿæ¸¬è©¦è³‡æ–™...")
    cmd = [
        'python', 'utils/generate_data.py',
        '-n', str(state['num_rows']),
        '-o', state['output'],
        '--normal_prob', str(state['normal_prob']),
        '--abnormal_prob', str(state['abnormal_prob']),
        '--null_prob', str(state['null_prob'])
    ]
    subprocess.run(cmd)
    state['data_generated'] = True
    return state

def anomaly_detection(state: WorkflowState, config=None):
    print("\n[Agentic Workflow] é–‹å§‹ç•°å¸¸æª¢æ¸¬...")
    try:
        # å‹•æ…‹å°å…¥ checking_agent
        sys.path.append('utils')
        from checking_agent import DataQualityAgent
        
        # å»ºç«‹ç•°å¸¸æª¢æ¸¬ä»£ç†
        agent = DataQualityAgent(state['output'])
        
        # è¼‰å…¥è³‡æ–™
        if not agent.load_data():
            print("[!] ç•°å¸¸æª¢æ¸¬ï¼šè³‡æ–™è¼‰å…¥å¤±æ•—")
            state['anomaly_checked'] = False
            return state
        
        # åŸ·è¡Œç•°å¸¸æª¢æ¸¬ï¼ˆçµåˆ rule-based å’Œ model-basedï¼‰
        summary = agent.check_all_rows_combined()
        
        # ä¿å­˜çµ±è¨ˆçµæœåˆ°æª”æ¡ˆä¾› LLM åˆ†æä½¿ç”¨
        agent.save_summary_to_file(summary, "utils/Data/total_stat.txt")
        
        print(f"\n[âœ“] ç•°å¸¸æª¢æ¸¬å®Œæˆï¼è™•ç†äº† {summary['total_rows']} è¡Œè³‡æ–™")
        state['anomaly_checked'] = True
        state['summary_saved'] = True  # è¨˜éŒ„çµ±è¨ˆæª”æ¡ˆå·²ä¿å­˜
        
    except Exception as e:
        print(f"[!] ç•°å¸¸æª¢æ¸¬å¤±æ•—: {e}")
        state['anomaly_checked'] = False
    
    return state

def llm_summary_analysis(state: WorkflowState, config=None):
    """ä½¿ç”¨ LLM åˆ†æçµ±è¨ˆæª”æ¡ˆä¸¦ç”¢ç”Ÿç¸½çµ"""
    print("\n[Agentic Workflow] ğŸ¤– LLM æ­£åœ¨åˆ†ææª¢æ¸¬çµæœ...")
    
    try:
        # è®€å–çµ±è¨ˆæª”æ¡ˆ
        stat_file_path = "utils/Data/total_stat.txt"
        if not os.path.exists(stat_file_path):
            print("[!] çµ±è¨ˆæª”æ¡ˆä¸å­˜åœ¨ï¼Œè·³é LLM åˆ†æ")
            state['llm_summary_generated'] = False
            return state
        
        with open(stat_file_path, 'r', encoding='utf-8') as f:
            stat_content = f.read()
        
        # å»ºç«‹åˆ†æ prompt
        try:
            analysis_prompt = f"""è«‹ä½œç‚ºå·¥æ¥­æ„Ÿæ¸¬å™¨ç•°å¸¸æª¢æ¸¬å°ˆå®¶ï¼Œåˆ†æä»¥ä¸‹æª¢æ¸¬çµ±è¨ˆå ±å‘Šï¼Œä¸¦æä¾›å°ˆæ¥­çš„ç¸½çµå’Œå»ºè­°ã€‚

æª¢æ¸¬çµ±è¨ˆå ±å‘Šå…§å®¹ï¼š
{stat_content}

è«‹æ ¹æ“šä»¥ä¸Šçµ±è¨ˆçµæœï¼Œæä¾›ä»¥ä¸‹åˆ†æï¼š

1. **ç³»çµ±æ•´é«”å¥åº·ç‹€æ³è©•ä¼°** (å„ªç§€/è‰¯å¥½/ä¸€èˆ¬/éœ€æ³¨æ„/åš´é‡)
2. **ä¸»è¦å•é¡Œè­˜åˆ¥** (åˆ—å‡ºæœ€åš´é‡çš„ 2-3 å€‹å•é¡Œ)
3. **æ ¹æœ¬åŸå› åˆ†æ** (å¯èƒ½çš„åŸå› æ¨æ¸¬)
4. **å…·é«”è¡Œå‹•å»ºè­°** (å„ªå…ˆç´šæ’åºçš„ä¿®å¾©æ­¥é©Ÿ)
5. **é é˜²æªæ–½å»ºè­°** (é¿å…æœªä¾†å†æ¬¡ç™¼ç”Ÿ)

è«‹ç”¨æ¸…æ¥šæ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ï¼Œé‡å°è¨­å‚™ç¶­è­·äººå“¡æä¾›å¯¦ç”¨çš„è¦‹è§£ã€‚å›ç­”è¦ç°¡æ½”æ˜äº†ï¼Œé‡é»çªå‡ºã€‚"""

            # å‘¼å«çµ±ä¸€çš„ LLM å‡½æ•¸
            llm_summary = Use_LLM(analysis_prompt, temperature=0.7)
            
            if llm_summary:  # ç¢ºèª LLM æœ‰å›æ‡‰
                print("\n" + "="*80)
                print("ğŸ¤– LLM æ™ºèƒ½åˆ†æç¸½çµ")
                print("="*80)
                print(llm_summary)
                print("="*80)
                state['llm_summary_generated'] = True
            else:
                raise Exception("LLM æ²’æœ‰å›æ‡‰")
            
        except Exception as e:
            print(f"[!] LLM åˆ†æå¤±æ•—: {e}")
            print("ğŸ“Š å°‡é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆæ‘˜è¦")
            
            # LLM å¤±æ•—æ™‚çš„ç°¡å–®æ‘˜è¦
            print("\n" + "="*60)
            print("ğŸ“Š æª¢æ¸¬çµæœåŸºæœ¬æ‘˜è¦")
            print("="*60)
            print(stat_content)
            print("="*60)
            
            state['llm_summary_generated'] = False
    
    except Exception as e:
        print(f"[!] è®€å–çµ±è¨ˆæª”æ¡ˆå¤±æ•—: {e}")
        state['llm_summary_generated'] = False
    
    return state

def user_query(state: WorkflowState, config=None):
    """è®“ä½¿ç”¨è€…è©¢å•é—œæ–¼æ•¸æ“šçš„å•é¡Œ"""
    print("\n" + "="*80)
    print("ğŸ“Š æ•¸æ“šæŸ¥è©¢åŠ©æ‰‹")
    print("="*80)
    print("æ‚¨å¯ä»¥è©¢å•é—œæ–¼ utils/Data/testing.csv çš„ä»»ä½•å•é¡Œï¼Œä¾‹å¦‚ï¼š")
    print("- æ•¸æ“šç¸½å…±æœ‰å¹¾è¡Œï¼Ÿ")
    print("- æº«åº¦çš„å¹³å‡å€¼æ˜¯å¤šå°‘ï¼Ÿ")
    print("- æœ‰å¤šå°‘å€‹ç•°å¸¸æ•¸æ“šï¼Ÿ")
    print("- è¼¸å…¥ 'quit' æˆ– 'çµæŸ' é€€å‡ºæŸ¥è©¢æ¨¡å¼")
    print("="*80)
    
    # ç²å–ç”¨æˆ¶å•é¡Œ
    user_question = input("\nğŸ¤” è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ: ").strip()
    
    if user_question.lower() in ['quit', 'çµæŸ', 'exit', 'é€€å‡º']:
        state['query_completed'] = True
        state['continue_asking'] = False
        print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨æ•¸æ“šæŸ¥è©¢åŠ©æ‰‹ï¼")
        return state
    
    state['user_question'] = user_question
    state['query_completed'] = False
    
    # ä½¿ç”¨ LLM åˆ¤æ–·æ˜¯å¦éœ€è¦ç”Ÿæˆä»£ç¢¼
    try:
        # å»ºç«‹åˆ¤æ–· prompt
        judge_prompt = f"""ä½ æ˜¯ä¸€å€‹æ•¸æ“šåˆ†æå°ˆå®¶ã€‚è«‹åˆ¤æ–·ä»¥ä¸‹ç”¨æˆ¶å•é¡Œæ˜¯å¦éœ€è¦ç”Ÿæˆ pandas ä»£ç¢¼ä¾†å›ç­”ã€‚

ç”¨æˆ¶å•é¡Œï¼š"{user_question}"

æ•¸æ“šæª”æ¡ˆï¼šutils/Data/testing.csv (åŒ…å«æ¬„ä½ï¼štimestamp, temp, pressure, vibration, label)

è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
Code_flag: <yes/no>
Response: <å¦‚æœä¸éœ€è¦ä»£ç¢¼ï¼Œè«‹ç›´æ¥å›ç­”å•é¡Œï¼›å¦‚æœéœ€è¦ä»£ç¢¼ï¼Œè«‹èªªæ˜å°‡è¦åŸ·è¡Œä»€éº¼åˆ†æ>

åˆ¤æ–·åŸå‰‡ï¼š
- å¦‚æœå•é¡Œéœ€è¦è¨ˆç®—ã€çµ±è¨ˆã€ç¹ªåœ–ã€æ•¸æ“šåˆ†æï¼Œå‰‡å›ç­” yes
- å¦‚æœå•é¡Œæ˜¯æ¦‚å¿µæ€§å•é¡Œã€ä¸€èˆ¬æ€§èªªæ˜ï¼Œå‰‡å›ç­” no ä¸¦ç›´æ¥è§£ç­”

ç¯„ä¾‹ï¼š
å•é¡Œï¼š"æ•¸æ“šæœ‰å¹¾è¡Œï¼Ÿ" â†’ Code_flag: yes
å•é¡Œï¼š"ä»€éº¼æ˜¯æº«åº¦æ„Ÿæ¸¬å™¨ï¼Ÿ" â†’ Code_flag: no
"""

        # ä½¿ç”¨çµ±ä¸€çš„ LLM å‡½æ•¸
        llm_output = Use_LLM(judge_prompt, temperature=0.3)
        
        # è§£æ LLM è¼¸å‡º
        import re
        code_flag_match = re.search(r'Code_flag:\s*(yes|no)', llm_output, re.IGNORECASE)
        response_match = re.search(r'Response:\s*(.*)', llm_output, re.DOTALL)
        
        if code_flag_match:
            state['code_flag'] = code_flag_match.group(1).lower()
        else:
            state['code_flag'] = 'yes'  # é è¨­éœ€è¦ä»£ç¢¼
        
        if response_match:
            state['llm_code_response'] = response_match.group(1).strip()
        else:
            state['llm_code_response'] = "æˆ‘å°‡ç‚ºæ‚¨åˆ†æé€™å€‹å•é¡Œã€‚"
        
        print(f"\nğŸ¤– åˆ†æçµæœï¼š{state['llm_code_response']}")
        
    except Exception as e:
        print(f"[!] LLM åˆ¤æ–·å¤±æ•—: {e}")
        state['code_flag'] = 'yes'  # é è¨­éœ€è¦ä»£ç¢¼
        state['llm_code_response'] = "æˆ‘å°‡å˜—è©¦ç”¨ä»£ç¢¼ä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚"
    
    return state

def code_exe(state: WorkflowState, config=None):
    """åŸ·è¡Œ LLM ç”Ÿæˆçš„ä»£ç¢¼æˆ–ç›´æ¥å›ç­”å•é¡Œ"""
    
    if state['code_flag'] == 'no':
        # ä¸éœ€è¦ä»£ç¢¼ï¼Œç›´æ¥é¡¯ç¤ºå›ç­”
        print("="*80)
        print(f"ğŸ’¡ å›ç­”ï¼š{state['llm_code_response']}")
        print("="*80)
    else:
        # éœ€è¦ç”Ÿæˆå’ŒåŸ·è¡Œä»£ç¢¼
        print("\nğŸ”§ æ­£åœ¨ç”Ÿæˆåˆ†æä»£ç¢¼...")
        
        try:
            # å»ºç«‹ä»£ç¢¼ç”Ÿæˆ prompt
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                if retry_count == 0:
                    code_prompt = f"""ä½ æ˜¯ä¸€å€‹ pandas æ•¸æ“šåˆ†æå°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹å•é¡Œç”Ÿæˆ Python ä»£ç¢¼ã€‚

ç”¨æˆ¶å•é¡Œï¼š"{state['user_question']}"
æ•¸æ“šæª”æ¡ˆï¼šutils/Data/testing.csv

æ•¸æ“šæ ¼å¼ï¼š
- timestamp: æ™‚é–“æˆ³è¨˜
- temp: æº«åº¦å€¼ (å¯èƒ½æœ‰ç©ºå€¼)
- pressure: å£“åŠ›å€¼ (å¯èƒ½æœ‰ç©ºå€¼)  
- vibration: æŒ¯å‹•å€¼ (å¯èƒ½æœ‰ç©ºå€¼)
- label: æ¨™ç±¤ (normal/abnormal)

è«‹ç”Ÿæˆç°¡æ½”çš„ Python ä»£ç¢¼ï¼ŒåŒ…å«ï¼š
1. å°å…¥å¿…è¦çš„åº« (pandas, numpy, matplotlib.pyplot as plt)
2. è®€å–æ•¸æ“š: df = pd.read_csv('utils/Data/testing.csv')
3. åˆ†ææ•¸æ“šä¸¦å›ç­”å•é¡Œ
4. å¦‚æœæ˜¯ç¹ªåœ–ï¼Œè«‹ä½¿ç”¨ matplotlib ä¸¦ä¿å­˜ç‚º PNG æª”æ¡ˆ
5. ç¢ºä¿ä»£ç¢¼å…·æœ‰éŒ¯èª¤è™•ç† (ä½¿ç”¨ try-except)
6. **é‡è¦ï¼šåœ¨æœ€å¾Œä¸€å®šè¦ç”¨ print() è¼¸å‡ºçµæœï¼Œè®“ç”¨æˆ¶èƒ½çœ‹åˆ°ç­”æ¡ˆ**

è«‹æ³¨æ„ï¼š
- ä¸è¦ä½¿ç”¨ `if __name__ == '__main__':` æ¢ä»¶
- ä¸è¦å®šç¾©å‡½æ•¸ï¼Œç›´æ¥å¯«åŸ·è¡Œä»£ç¢¼
- ä»£ç¢¼æ‡‰è©²ç›´æ¥åŸ·è¡Œä¸¦ç”¢ç”Ÿè¼¸å‡º

ç¯„ä¾‹æ ¼å¼ï¼š
```python
import pandas as pd
df = pd.read_csv('utils/Data/testing.csv')
# åˆ†æä»£ç¢¼...
print(f"çµæœ: {{value}}")
```

è«‹åªå›å‚³å¯åŸ·è¡Œçš„ Python ä»£ç¢¼ï¼Œä¸è¦åŒ…å«é¡å¤–èªªæ˜ï¼š"""
                else:
                    # é‡è©¦æ™‚åŒ…å«éŒ¯èª¤è¨Šæ¯
                    code_prompt = f"""å‰ä¸€æ¬¡ä»£ç¢¼åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯ï¼š
{state.get('code_error', 'æœªçŸ¥éŒ¯èª¤')}

è«‹ä¿®æ­£ä»£ç¢¼ä¸¦é‡æ–°ç”Ÿæˆã€‚

åŸå§‹å•é¡Œï¼š"{state['user_question']}"
æ•¸æ“šæª”æ¡ˆï¼šutils/Data/testing.csv

è«‹ç”Ÿæˆä¿®æ­£å¾Œçš„å®Œæ•´ Python ä»£ç¢¼ï¼š"""

                # ä½¿ç”¨çµ±ä¸€çš„ LLM å‡½æ•¸
                generated_code = Use_LLM(code_prompt, temperature=0.3).strip()
                
                # æ¸…ç†ä»£ç¢¼ï¼ˆç§»é™¤ markdown æ ¼å¼ï¼‰
                import re
                code_match = re.search(r'```python\n(.*?)\n```', generated_code, re.DOTALL)
                if code_match:
                    generated_code = code_match.group(1)
                elif '```' in generated_code:
                    # è™•ç†å…¶ä»– markdown æ ¼å¼
                    generated_code = re.sub(r'```.*?\n', '', generated_code)
                    generated_code = re.sub(r'\n```', '', generated_code)
                
                print("="*80)
                print("ğŸ”§ ç”Ÿæˆçš„ä»£ç¢¼ï¼š")
                print("="*80)
                print(generated_code)
                print("="*80)
                
                # åŸ·è¡Œä»£ç¢¼
                try:
                    print("âš¡ åŸ·è¡Œä¸­...")
                    print("="*80)
                    
                    # å»ºç«‹åŸ·è¡Œç’°å¢ƒ
                    import sys
                    import io
                    
                    # æ•ç²æ¨™æº–è¼¸å‡º
                    old_stdout = sys.stdout
                    captured_output = io.StringIO()
                    
                    try:
                        # å»ºç«‹åŸ·è¡Œç’°å¢ƒï¼ŒåŒ…å«å®Œæ•´çš„å…¨å±€è®Šæ•¸
                        exec_globals = {
                            '__builtins__': __builtins__,
                            '__name__': '__main__',  # è¨­å®š __name__ ç‚º '__main__'
                            'pandas': __import__('pandas'),
                            'numpy': __import__('numpy'),
                            'matplotlib': __import__('matplotlib'),
                            'plt': __import__('matplotlib.pyplot'),
                            'print': print,  # ç¢ºä¿ print å‡½æ•¸å¯ç”¨
                            'sys': sys,
                            'os': __import__('os'),
                        }
                        
                        # è¨­å®š matplotlib å¾Œç«¯
                        exec_globals['matplotlib'].use('Agg')
                        
                        # é‡å®šå‘è¼¸å‡ºä»¥æ•ç²çµæœ
                        sys.stdout = captured_output
                        
                        # åŸ·è¡Œä»£ç¢¼
                        exec(generated_code, exec_globals)
                        
                        # æ¢å¾©æ¨™æº–è¼¸å‡º
                        sys.stdout = old_stdout
                        
                        # ç²å–æ•ç²çš„è¼¸å‡º
                        output_content = captured_output.getvalue()
                        
                        # é¡¯ç¤ºåŸ·è¡Œçµæœ
                        if output_content.strip():
                            print("ğŸ“Š åŸ·è¡Œçµæœï¼š")
                            print(output_content)
                        else:
                            print("âœ… ä»£ç¢¼åŸ·è¡Œå®Œæˆï¼ˆç„¡è¼¸å‡ºï¼‰")
                        
                    finally:
                        # ç¢ºä¿æ¢å¾©æ¨™æº–è¼¸å‡º
                        sys.stdout = old_stdout
                        captured_output.close()
                    
                    print("="*80)
                    print("âœ… ä»£ç¢¼åŸ·è¡ŒæˆåŠŸï¼")
                    state['code_execution_result'] = "åŸ·è¡ŒæˆåŠŸ"
                    state['code_error'] = ""
                    break
                    
                except Exception as e:
                    retry_count += 1
                    error_msg = str(e)
                    print(f"âŒ ä»£ç¢¼åŸ·è¡Œå¤±æ•— (ç¬¬ {retry_count} æ¬¡): {error_msg}")
                    state['code_error'] = error_msg
                    
                    if retry_count >= max_retries:
                        print("âš ï¸ å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œç„¡æ³•åŸ·è¡Œä»£ç¢¼")
                        state['code_execution_result'] = f"åŸ·è¡Œå¤±æ•—: {error_msg}"
                        break
                    else:
                        print(f"ğŸ”„ æ­£åœ¨é‡è©¦... ({retry_count + 1}/{max_retries})")
        
        except Exception as e:
            print(f"[!] ä»£ç¢¼ç”Ÿæˆå¤±æ•—: {e}")
            state['code_execution_result'] = f"ç”Ÿæˆå¤±æ•—: {e}"
    
    # è©¢å•æ˜¯å¦ç¹¼çºŒ
    print("\n" + "="*80)
    continue_choice = input("â“ æ˜¯å¦è¦ç¹¼çºŒè©¢å•å…¶ä»–å•é¡Œï¼Ÿ(y/n): ").strip().lower()
    
    if continue_choice in ['y', 'yes', 'æ˜¯', 'ç¹¼çºŒ']:
        state['continue_asking'] = True
        state['query_completed'] = False
    else:
        state['continue_asking'] = False
        state['query_completed'] = True
        print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨æ•¸æ“šæŸ¥è©¢åŠ©æ‰‹ï¼")
    
    return state

def cleanup_files(state: WorkflowState, config=None):
    """æ¸…ç†æš«å­˜æª”æ¡ˆ"""
    try:
        stat_file_path = "utils/Data/total_stat.txt"
        if os.path.exists(stat_file_path):
            os.remove(stat_file_path)
    except Exception as e:
        print(f"[!] æ¸…ç†æª”æ¡ˆå¤±æ•—: {e}")
    
    return state

def end_node(state: WorkflowState, config=None):
    print("\n[âœ“] Agentic Workflow å®Œæˆï¼\n")
    return state

def build_workflow():
    from langgraph.graph import StateGraph, END
    
    # Define a new graph with our state
    workflow = StateGraph(WorkflowState)
    
    # 1. Add our nodes 
    workflow.add_node("check_file", check_file)
    workflow.add_node("prompt_params", prompt_params)
    workflow.add_node("user_intent_analysis", user_intent_analysis)
    workflow.add_node("generate_data", generate_data)
    workflow.add_node("anomaly_detection", anomaly_detection)
    workflow.add_node("llm_summary_analysis", llm_summary_analysis)
    workflow.add_node("user_query", user_query)
    workflow.add_node("code_exe", code_exe)
    workflow.add_node("cleanup_files", cleanup_files)
    workflow.add_node("finish", end_node)
    
    # 2. Set the entrypoint as `check_file`, this is the first node called
    workflow.set_entry_point("check_file")
    
    # 3. Add a conditional edge after the `check_file` node is called.
    def check_file_branch(state):
        return "anomaly_detection" if state["file_exists"] else "prompt_params"
    
    workflow.add_conditional_edges(
        "check_file",
        check_file_branch,
        {
            "anomaly_detection": "anomaly_detection",
            "prompt_params": "prompt_params",
        },
    )
    
    # 4. Add conditional edge after prompt_params
    def prompt_params_branch(state):
        return "generate_data" if state["params_confirmed"] else "user_intent_analysis"
    
    workflow.add_conditional_edges(
        "prompt_params",
        prompt_params_branch,
        {
            "generate_data": "generate_data",
            "user_intent_analysis": "user_intent_analysis",
        },
    )
    
    # 5. Add conditional edge after user_intent_analysis
    def intent_analysis_branch(state):
        return "generate_data" if state["params_confirmed"] else "user_intent_analysis"
    
    workflow.add_conditional_edges(
        "user_intent_analysis",
        intent_analysis_branch,
        {
            "generate_data": "generate_data",
            "user_intent_analysis": "user_intent_analysis",  # å¯ä»¥å¾ªç’°å›è‡ªå·±
        },
    )
    
    # 6. Add normal edges after other nodes are called
    workflow.add_edge("generate_data", "anomaly_detection")
    workflow.add_edge("anomaly_detection", "llm_summary_analysis")
    workflow.add_edge("llm_summary_analysis", "user_query")
    
    # 7. Add conditional edge after code_exe
    def code_exe_branch(state):
        return "user_query" if state.get("continue_asking", False) else "cleanup_files"
    
    workflow.add_conditional_edges(
        "code_exe",
        code_exe_branch,
        {
            "user_query": "user_query",
            "cleanup_files": "cleanup_files",
        },
    )
    
    # 6. Add conditional edge after user_query
    def user_query_branch(state):
        return "cleanup_files" if state.get("query_completed", False) else "code_exe"
    
    workflow.add_conditional_edges(
        "user_query",
        user_query_branch,
        {
            "code_exe": "code_exe",
            "cleanup_files": "cleanup_files",
        },
    )
    workflow.add_edge("cleanup_files", "finish")
    
    # Now we can compile and visualize our graph
    return workflow.compile()

def save_workflow_diagram():
    """ç”Ÿæˆä¸¦ä¿å­˜å·¥ä½œæµç¨‹åœ–"""
    import os
    import subprocess
    import base64
    import requests
    from PIL import Image, ImageDraw, ImageFont
    
    print("\nğŸ¨ æ­£åœ¨ç”Ÿæˆå·¥ä½œæµç¨‹åœ–...")
    
    # å»ºç«‹å·¥ä½œæµç¨‹
    app = build_workflow()
    
    try:
        # å˜—è©¦ä½¿ç”¨ LangGraph å…§å»ºçš„ mermaid åŠŸèƒ½
        mermaid_code = app.get_graph(xray=True).draw_mermaid()
        print("âœ… æˆåŠŸç”Ÿæˆ Mermaid ä»£ç¢¼")
        
        # å¤šå±¤æ¬¡çš„ä¿å­˜ç­–ç•¥
        saved = False
        
        if not saved:
            try:
                print("ğŸŒ å˜—è©¦ä½¿ç”¨ mermaid.ink API...")
                
                # ç·¨ç¢¼ mermaid ä»£ç¢¼
                encoded = base64.urlsafe_b64encode(mermaid_code.encode('utf-8')).decode('ascii')
                
                # è«‹æ±‚ API
                url = f"https://mermaid.ink/img/{encoded}"
                response = requests.get(url, timeout=10)
                
                if response.status_code == 200:
                    os.makedirs('img', exist_ok=True)
                    with open('img/workflow.png', 'wb') as f:
                        f.write(response.content)
                    print("âœ… æˆåŠŸä½¿ç”¨ mermaid.ink API ç”Ÿæˆåœ–ç‰‡")
                    saved = True
                else:
                    print(f"âš ï¸ mermaid.ink API å¤±æ•—: {response.status_code}")
                    
            except Exception as e:
                print(f"âš ï¸ mermaid.ink API éŒ¯èª¤: {e}")
        
                
            except Exception as e:
                print(f"âš ï¸ ç°¡å–®åœ–è¡¨ç”Ÿæˆå¤±æ•—: {e}")
        
        # ä¿å­˜ mermaid ä»£ç¢¼åˆ°æª”æ¡ˆ
        try:
            os.makedirs('img', exist_ok=True)
            with open('img/workflow.mmd', 'w', encoding='utf-8') as f:
                f.write(mermaid_code)
            print("ğŸ’¾ Mermaid ä»£ç¢¼å·²ä¿å­˜åˆ° img/workflow.mmd")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ Mermaid ä»£ç¢¼å¤±æ•—: {e}")
        
        if saved:
            print("ğŸ‰ å·¥ä½œæµç¨‹åœ–å·²æˆåŠŸä¿å­˜åˆ° img/workflow.png")
            print("ğŸ“„ Mermaid ä»£ç¢¼å·²ä¿å­˜åˆ° img/workflow.mmd")
        else:
            print("âŒ ç„¡æ³•ç”Ÿæˆå·¥ä½œæµç¨‹åœ–ï¼Œä½† Mermaid ä»£ç¢¼å·²ä¿å­˜")
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå·¥ä½œæµç¨‹åœ–å¤±æ•—: {e}")
        print("è«‹æª¢æŸ¥æ˜¯å¦å·²å®‰è£å¿…è¦çš„å¥—ä»¶")

def main():
    print("=== Agentic AI Workflow æª¢æŸ¥æ¸¬è©¦è³‡æ–™ ===")
    if '-p' in sys.argv:
        save_workflow_diagram()
        return
    
    # åˆå§‹åŒ– state - å› ç‚º TypedDict ä¸èƒ½ç›´æ¥å¯¦ä¾‹åŒ–ï¼Œä½¿ç”¨ dict
    state = {
        'num_rows': 300,
        'normal_prob': 0.95,
        'abnormal_prob': 0.3,
        'null_prob': 0.05,
        'file_exists': False,
        'data_path': '',
        'data_generated': False,
        'output': '',
        'anomaly_checked': False,
        'user_input': '',
        'llm_response': '',
        'params_confirmed': False,
        'conversation_active': False,
        'summary_saved': False,
        'llm_summary_generated': False,
        'user_question': '',
        'code_flag': '',
        'llm_code_response': '',
        'code_execution_result': '',
        'code_error': '',
        'query_completed': False,
        'continue_asking': False
    }
    
    app = build_workflow()
    app.invoke(state)

if __name__ == "__main__":
    main()